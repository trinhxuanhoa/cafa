import torch
from transformers import AutoTokenizer, AutoModel
from Bio import SeqIO
import os
from tqdm import tqdm
MODEL_NAME = "facebook/esm2_t33_650M_UR50D" 

FASTA_FILE = "data/raw/train_sequences.fasta"
SAVE_PATH = "data/embeddings/train_embeddings_t33.pt"

BATCH_SIZE = 16 

MAX_LEN = 1024 

def get_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

def main():
    device = get_device()
    print(f"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {device}")
    

    print(f"ƒêang t·∫£i model {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModel.from_pretrained(MODEL_NAME)
       
    if device.type == 'cuda':
        model = model.half()
        print("‚ö° ƒê√£ b·∫≠t ch·∫ø ƒë·ªô FP16 (Half Precision) ƒë·ªÉ tƒÉng t·ªëc.")
    
    model = model.to(device)
    model.eval()

    # x·ª≠ l√Ω ƒëa GPU 
    if torch.cuda.device_count() > 1:
        print(f"Ph√°t hi·ªán {torch.cuda.device_count()} GPUs. ƒêang k√≠ch ho·∫°t ch·∫°y song song.")
        model = torch.nn.DataParallel(model)

    #ƒë·ªçc d·ªØ li·ªáu
    print("üìñ ƒêang ƒë·ªçc file FASTA...")
    sequences = []
    ids = []
    # ram y·∫øu th√¨ ph·∫£i ƒë·ªçc t·ª´ng d√≤ng
    for record in SeqIO.parse(FASTA_FILE, "fasta"):
        ids.append(record.id)
        sequences.append(str(record.seq))
    
    print(f"T·ªïng s·ªë protein c·∫ßn x·ª≠ l√Ω: {len(sequences)}")
    
    embeddings_dict = {}

    with torch.no_grad():
        for i in tqdm(range(0, len(sequences), BATCH_SIZE), desc="Creating Embeddings"):
            batch_seqs = sequences[i : i + BATCH_SIZE]
            batch_ids = ids[i : i + BATCH_SIZE]

            inputs = tokenizer(
                batch_seqs, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=MAX_LEN
            ).to(device)

            outputs = model(**inputs)
            
            token_embeddings = outputs.last_hidden_state
            attention_mask = inputs['attention_mask']
            
            # m·ªü r·ªông mask
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            
            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
            
            batch_embeddings = sum_embeddings / sum_mask
                    
            for j, seq_id in enumerate(batch_ids):
                embeddings_dict[seq_id] = batch_embeddings[j].float().cpu()

    print(f" ƒêang l∆∞u file k·∫øt qu·∫£ v√†o {SAVE_PATH}...")
    torch.save(embeddings_dict, SAVE_PATH)
    print("Xong")

if __name__ == "__main__":
    main()
