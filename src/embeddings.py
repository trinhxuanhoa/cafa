import torch
from transformers import AutoTokenizer, AutoModel
from Bio import SeqIO
import os
from tqdm import tqdm

# --- Cáº¤U HÃŒNH CHO SERVER (HIGH PERFORMANCE) ---
# 1. Chá»n Model tá»‘t nháº¥t cÃ³ thá»ƒ cháº¡y á»•n Ä‘á»‹nh (t33 lÃ  chuáº©n vÃ ng cho CAFA)
# Náº¿u server báº¡n lÃ  A100 (40GB/80GB VRAM), báº¡n cÃ³ thá»ƒ thá»­ báº£n "esm2_t36_3B_UR50D" (3 tá»· tham sá»‘)
MODEL_NAME = "facebook/esm2_t33_650M_UR50D" 

FASTA_FILE = "data/raw/train_sequences.fasta"
SAVE_PATH = "data/embeddings/train_embeddings_t33.pt"

# TÄƒng Batch size tÃ¹y GPU:
# - GPU 16GB (T4, 3060): Äá»ƒ khoáº£ng 8-16
# - GPU 24GB (3090, 4090): Äá»ƒ khoáº£ng 32
# - GPU 40GB/80GB (A100): Äá»ƒ 64 hoáº·c cao hÆ¡n
BATCH_SIZE = 16 

MAX_LEN = 1024 
# ---------------------------------------------

def get_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

def main():
    device = get_device()
    print(f"ğŸš€ Äang cháº¡y trÃªn thiáº¿t bá»‹: {device}")
    
    if device.type == 'cpu':
        print("âš ï¸ Cáº¢NH BÃO: Báº¡n Ä‘ang cháº¡y model lá»›n trÃªn CPU. Sáº½ Ráº¤T CHáº¬M. HÃ£y Ä‘áº£m báº£o server cÃ³ GPU.")

    print(f"ğŸ“¥ Äang táº£i model 'xá»‹n' {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModel.from_pretrained(MODEL_NAME)
    
    # Ká»¸ THUáº¬T QUAN TRá»ŒNG: Chuyá»ƒn sang FP16 (Half Precision)
    # GiÃºp cháº¡y nhanh hÆ¡n vÃ  giáº£m VRAM
    if device.type == 'cuda':
        model = model.half()
        print("âš¡ ÄÃ£ báº­t cháº¿ Ä‘á»™ FP16 (Half Precision) Ä‘á»ƒ tÄƒng tá»‘c.")
    
    model = model.to(device)
    model.eval()

    # Xá»­ lÃ½ Ä‘a GPU (Náº¿u server cÃ³ nhiá»u hÆ¡n 1 GPU)
    if torch.cuda.device_count() > 1:
        print(f"ğŸ”¥ PhÃ¡t hiá»‡n {torch.cuda.device_count()} GPUs. Äang kÃ­ch hoáº¡t cháº¡y song song (DataParallel).")
        model = torch.nn.DataParallel(model)

    # Äá»c dá»¯ liá»‡u
    print("ğŸ“– Äang Ä‘á»c file FASTA...")
    sequences = []
    ids = []
    # LÆ°u Ã½: Náº¿u RAM server yáº¿u (<16GB), Ä‘oáº¡n nÃ y cÃ³ thá»ƒ cáº§n tá»‘i Æ°u Ä‘á»c tá»«ng dÃ²ng
    for record in SeqIO.parse(FASTA_FILE, "fasta"):
        ids.append(record.id)
        sequences.append(str(record.seq))
    
    print(f"ğŸ“Š Tá»•ng sá»‘ protein cáº§n xá»­ lÃ½: {len(sequences)}")
    
    embeddings_dict = {}

    with torch.no_grad():
        for i in tqdm(range(0, len(sequences), BATCH_SIZE), desc="Creating Embeddings"):
            batch_seqs = sequences[i : i + BATCH_SIZE]
            batch_ids = ids[i : i + BATCH_SIZE]

            inputs = tokenizer(
                batch_seqs, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=MAX_LEN
            ).to(device)

            # Láº¥y output tá»« model
            outputs = model(**inputs)
            
            # Láº¥y embedding (Mean Pooling)
            token_embeddings = outputs.last_hidden_state
            attention_mask = inputs['attention_mask']
            
            # Má»Ÿ rá»™ng mask Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘Ãºng kÃ­ch thÆ°á»›c
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            
            # TÃ­nh tá»•ng vÃ  chia trung bÃ¬nh
            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
            
            batch_embeddings = sum_embeddings / sum_mask
            
            # Quan trá»ng: Chuyá»ƒn vá» CPU vÃ  float32 Ä‘á»ƒ lÆ°u trá»¯ an toÃ n
            # (Giá»¯ file .pt á»Ÿ dáº¡ng float32 Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch tá»‘t nháº¥t vá»›i code train sau nÃ y)
            for j, seq_id in enumerate(batch_ids):
                embeddings_dict[seq_id] = batch_embeddings[j].float().cpu()

    print(f"ğŸ’¾ Äang lÆ°u file káº¿t quáº£ (náº·ng khoáº£ng 2-4GB) vÃ o {SAVE_PATH}...")
    torch.save(embeddings_dict, SAVE_PATH)
    print("âœ… Xong! Báº¡n Ä‘Ã£ cÃ³ bá»™ embedding cháº¥t lÆ°á»£ng cao nháº¥t.")

if __name__ == "__main__":
    main()