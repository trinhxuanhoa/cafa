import torch
from transformers import AutoTokenizer, AutoModel
from Bio import SeqIO
import os
from tqdm import tqdm
MODEL_NAME = "facebook/esm2_t33_650M_UR50D" 

FASTA_FILE = "data/raw/train_sequences.fasta"
SAVE_PATH = "data/embeddings/train_embeddings_t33.pt"

BATCH_SIZE = 16 

MAX_LEN = 1024 
# ---------------------------------------------

def get_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

def main():
    device = get_device()
    print(f"Äang cháº¡y trÃªn thiáº¿t bá»‹: {device}")
    

    print(f"Äang táº£i model {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModel.from_pretrained(MODEL_NAME)
       
    if device.type == 'cuda':
        model = model.half()
        print("âš¡ ÄÃ£ báº­t cháº¿ Ä‘á»™ FP16 (Half Precision) Ä‘á»ƒ tÄƒng tá»‘c.")
    
    model = model.to(device)
    model.eval()

    # Xá»­ lÃ½ Ä‘a GPU 
    if torch.cuda.device_count() > 1:
        print(f"PhÃ¡t hiá»‡n {torch.cuda.device_count()} GPUs. Äang kÃ­ch hoáº¡t cháº¡y song song.")
        model = torch.nn.DataParallel(model)

    # Äá»c dá»¯ liá»‡u
    print("ğŸ“– Äang Ä‘á»c file FASTA...")
    sequences = []
    ids = []
    # LÆ°u Ã½: Náº¿u RAM server yáº¿u (<16GB), Ä‘oáº¡n nÃ y cÃ³ thá»ƒ cáº§n tá»‘i Æ°u Ä‘á»c tá»«ng dÃ²ng
    for record in SeqIO.parse(FASTA_FILE, "fasta"):
        ids.append(record.id)
        sequences.append(str(record.seq))
    
    print(f"Tá»•ng sá»‘ protein cáº§n xá»­ lÃ½: {len(sequences)}")
    
    embeddings_dict = {}

    with torch.no_grad():
        for i in tqdm(range(0, len(sequences), BATCH_SIZE), desc="Creating Embeddings"):
            batch_seqs = sequences[i : i + BATCH_SIZE]
            batch_ids = ids[i : i + BATCH_SIZE]

            inputs = tokenizer(
                batch_seqs, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=MAX_LEN
            ).to(device)

            # Láº¥y output tá»« model
            outputs = model(**inputs)
            
            # Láº¥y embedding (Mean Pooling)
            token_embeddings = outputs.last_hidden_state
            attention_mask = inputs['attention_mask']
            
            # Má»Ÿ rá»™ng mask Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘Ãºng kÃ­ch thÆ°á»›c
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            
            # TÃ­nh tá»•ng vÃ  chia trung bÃ¬nh
            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
            
            batch_embeddings = sum_embeddings / sum_mask
                    
            for j, seq_id in enumerate(batch_ids):
                embeddings_dict[seq_id] = batch_embeddings[j].float().cpu()

    print(f" Äang lÆ°u file káº¿t quáº£ vÃ o {SAVE_PATH}...")
    torch.save(embeddings_dict, SAVE_PATH)
    print("Xong")

if __name__ == "__main__":
    main()